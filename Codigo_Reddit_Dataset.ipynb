{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jibarletta/PAECIS/blob/main/Codigo_Reddit_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReadMe: A modo de introducción a las explicaciones que encontrarán en este código, nos parece relevante aclarar que, luego de pruebas y modificaciones hemos decidido armar un sólo dataset que agrupe los 10 post más votados de cada día y todos sus correspondientes comentarios.\n",
        "Se scrapea esta información por partes. Primero los temas o posts (los cuales antes eran separados en un data set aparte) y posteriormente todos los comentarios de ese tema. Luego hemos entendido que por operatoria convenía juntarnos en un gran dataset para su posterior análisis.\n",
        "\n",
        "El código puede ser observado en el siguiente [enlace](https://github.com/jibarletta/mokugeki/blob/master/main.py)."
      ],
      "metadata": {
        "id": "PE6c9DNRxzZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero importamos las librerías necesarias."
      ],
      "metadata": {
        "id": "hP7olHEyaea2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "from datetime import datetime as dt\n",
        "from credenciales import redditauth\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "AYsFbNtVaaST",
        "outputId": "b022d780-5b27-4a55-cd0a-ea329296f205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'credenciales'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8c1f62685cc5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcredenciales\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mredditauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'credenciales'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego validamos las credenciales"
      ],
      "metadata": {
        "id": "yPkeUpbTakES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = redditauth()"
      ],
      "metadata": {
        "id": "beBb1M5ZbXZ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "3b41874f-31e8-474a-a4c6-6137a649adf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'redditauth' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3017e3453bc3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreddit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mredditauth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'redditauth' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se genera un datestamp para su posterior uso. Será muy necesario para clasificar datos y archivos."
      ],
      "metadata": {
        "id": "GYj-Pwgnbbyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename_date = dt.now().strftime('%Y-%m-%d')  # -%H-%M removido del filename"
      ],
      "metadata": {
        "id": "jecj763Kbar2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero armamos un diccionario con los 10 posts más populares del subreddit de \"Argentina\"."
      ],
      "metadata": {
        "id": "PA4gd_0sssvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "post_dict = {\"id\": [],\n",
        "             \"context\": [],\n",
        "             \"text\": [],\n",
        "             \"upv\": [],\n",
        "             \"author\": [],\n",
        "             \"date\": [],\n",
        "             \"type\": [],\n",
        "             }\n",
        "arg_subreddit = reddit.subreddit('Argentina')\n",
        "for post in arg_subreddit.top(limit=10, time_filter='day'):\n",
        "    post_dict[\"id\"].append(post.id)\n",
        "    post_dict[\"context\"].append(post.id)\n",
        "    post_dict[\"text\"].append(post.title)\n",
        "    post_dict[\"upv\"].append(post.score)\n",
        "    post_dict[\"author\"].append(post.author)\n",
        "    post_dict[\"date\"].append(filename_date)\n",
        "    post_dict['type'].append('post')"
      ],
      "metadata": {
        "id": "Qnyxrn6tssaC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "5b2a62b6-f676-4d29-e3bf-7e2ee68f72d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'reddit' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c4c4f51b73dc>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m              \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m              }\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0marg_subreddit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Argentina'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_subreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpost_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reddit' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se convierte el diccionario en un dataframe. Se lo guarda en CSV. Luego se lo formatea para concatenarlo con los posteriores datos que se sacarán del subreddit. También se arma un csv aparte sólo con los 10 temas del día."
      ],
      "metadata": {
        "id": "I-VTURiftWJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se convierte el diccionario en un data frame.\n",
        "temas = pd.DataFrame(post_dict)\n",
        "temas.to_csv(f\"{Path.cwd()}/Bases/Temas/Reddit_Arg_Temas{filename_date}.csv\")\n",
        "dataset = pd.read_csv(f\"{Path.cwd()}/Bases/Dataset/Red_Arg_Dataset.csv\", index_col=0)\n",
        "nuevods = pd.concat([dataset, temas], sort=False)\n",
        "nuevods.reset_index(inplace=True, drop=True)\n",
        "nuevods.to_csv(f\"{Path.cwd()}/Bases/Dataset/Red_Arg_Dataset.csv\")"
      ],
      "metadata": {
        "id": "Ke68oYe5tV9_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "34ac5fc3-cb1a-4f4b-c5fe-6d56e4275122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-dacff4dea32a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Se convierte el diccionario en un data frame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtemas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtemas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{Path.cwd()}/Bases/Temas/Reddit_Arg_Temas{filename_date}.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{Path.cwd()}/Bases/Dataset/Red_Arg_Dataset.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnuevods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se realiza el mismo procedimiento para los comentarios de cada uno de los post del diccionario anterior temas. Se concatena al dataset general."
      ],
      "metadata": {
        "id": "JVqHqlMCtxZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mismo procedimiento anterior pero para los comentarios de cada post.\n",
        "lista_ids = temas[\"id\"]\n",
        "for postid in lista_ids:\n",
        "    comment_dict = {\"id\": [],\n",
        "                    \"context\": [],\n",
        "                    \"text\": [],\n",
        "                    \"upv\": [],\n",
        "                    \"author\": [],\n",
        "                    \"date\": [],\n",
        "                    \"type\": [],\n",
        "                    }\n",
        "    posteo = reddit.submission(postid)\n",
        "    posteo.comments.replace_more(limit=0)\n",
        "    for comentario in posteo.comments.list():\n",
        "        if comentario.author == 'empleadoEstatalBot':  # Elimina el bot que postea la noticia en caso de existir.\n",
        "            continue\n",
        "        comment_dict[\"id\"].append(comentario.id)\n",
        "        p_split = comentario.parent_id.split('_')\n",
        "        idctx = p_split[1]\n",
        "        comment_dict['context'].append(idctx)\n",
        "        comment_dict[\"text\"].append(comentario.body)\n",
        "        comment_dict[\"upv\"].append(comentario.score)\n",
        "        comment_dict[\"author\"].append(comentario.author)\n",
        "        comment_dict['date'].append(filename_date)\n",
        "        comment_dict['type'].append('comment')"
      ],
      "metadata": {
        "id": "NXHkSVCAtxBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta última sección de termina de concatenar el dataset final y se separa aparte un archivo sólo con los comentarios del correpsondiente post."
      ],
      "metadata": {
        "id": "xTeuKcfdEXKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    dataset = pd.read_csv(f\"{Path.cwd()}/Bases/Dataset/Red_Arg_Dataset.csv\", index_col=0)\n",
        "    comms = pd.DataFrame(comment_dict)\n",
        "    nuevods = pd.concat([dataset, comms], sort=False)\n",
        "    nuevods.reset_index(inplace=True, drop=True)\n",
        "    nuevods.to_csv(f\"{Path.cwd()}/Bases/Dataset/Red_Arg_Dataset.csv\")\n",
        "    comms.to_csv(f\"{Path.cwd()}/Bases/Comentarios/Red_Arg_Comm_Post-{postid}-{filename_date}.csv\")\n",
        "    archivo = f\"Red_Arg_Comm_Post-{postid}-{filename_date}.csv\""
      ],
      "metadata": {
        "id": "Xf-cJtdyEYH2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}