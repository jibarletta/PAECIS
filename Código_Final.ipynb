{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jibarletta/PAECIS/blob/main/C%C3%B3digo_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy2Dgl_4cQB9"
      },
      "outputs": [],
      "source": [
        "#Instalación de paquetes necesarios para los procedimientos de extracción, preprocesamiento y procesamiento#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fase 1: Extracción"
      ],
      "metadata": {
        "id": "_YvL34iDdKua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#A continuación se genera un datastamp para su posterior uso. Será muy necesario para clasificar datos y archivos#\n",
        "filename_date = dt.now().strftime('%Y-%m-%d')  # -%H-%M removido del filename"
      ],
      "metadata": {
        "id": "qKuFeIQjdPER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "from datetime import datetime as dt\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "!pip install bertopic\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "import numpy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim import corpora\n",
        "from bertopic import BERTopic\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models import CoherenceModel\n",
        "from sklearn.metrics import silhouette_score"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bxKJNK50crt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se convierte el diccionario en un dataframe. Se lo guarda en CVS. Luego se lo formatea para concatenarlo con los posteriores datos que se sacarán de Reedit. También se arma un CSV aparte solo con los 10 temas del día#\n",
        "temas = pd.DataFrame(post_dict)\n",
        "temas.to_csv(f\"{Path.cwd()}/Bases/Temas/Reddit_Arg_Temas{filename_date}.csv\")\n",
        "dataset = pd.read_csv(f\"{Path.cwd()}/Bases/Dataset/Red_Arg_Dataset.csv\", index_col=0)\n",
        "nuevods = pd.concat([dataset, temas], sort=False)\n",
        "nuevods.reset_index(inplace=True, drop=True)\n",
        "nuevods.to_csv(f\"{Path.cwd()}/Bases/Dataset/Red_Arg_Dataset.csv\")"
      ],
      "metadata": {
        "id": "G9kWQTWffyTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Se realiza el mismo procedimiento para los comentarios de cada uno de los post del diccionario anterior. Se concatena al dataset general\n",
        "lista_ids = temas[\"id\"]\n",
        "for postid in lista_ids:\n",
        "    comment_dict = {\"id\": [],\n",
        "                    \"context\": [],\n",
        "                    \"text\": [],\n",
        "                    \"upv\": [],\n",
        "                    \"author\": [],\n",
        "                    \"date\": [],\n",
        "                    \"type\": [],\n",
        "                    }\n",
        "    posteo = reddit.submission(postid)\n",
        "    posteo.comments.replace_more(limit=0)\n",
        "    for comentario in posteo.comments.list():\n",
        "        if comentario.author == 'empleadoEstatalBot':  # Elimina el bot que postea la noticia en caso de existir.\n",
        "            continue\n",
        "        comment_dict[\"id\"].append(comentario.id)\n",
        "        p_split = comentario.parent_id.split('_')\n",
        "        idctx = p_split[1]\n",
        "        comment_dict['context'].append(idctx)\n",
        "        comment_dict[\"text\"].append(comentario.body)\n",
        "        comment_dict[\"upv\"].append(comentario.score)\n",
        "        comment_dict[\"author\"].append(comentario.author)\n",
        "        comment_dict['date'].append(filename_date)\n",
        "        comment_dict['type'].append('comment')"
      ],
      "metadata": {
        "id": "RMIuUU5Vgoek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#En esta última sección se termina de concatenar el dataset final y se separa aparte un archivo sólo con los comentarios del correspondiente post\n",
        "    dataset = pd.read_csv(f\"{Path.cwd()}/Bases/Dataset/Red_Arg_Dataset.csv\", index_col=0)\n",
        "    comms = pd.DataFrame(comment_dict)\n",
        "    nuevods = pd.concat([dataset, comms], sort=False)\n",
        "    nuevods.reset_index(inplace=True, drop=True)\n",
        "    nuevods.to_csv(f\"{Path.cwd()}/Bases/Dataset/Red_Arg_Dataset.csv\")\n",
        "    comms.to_csv(f\"{Path.cwd()}/Bases/Comentarios/Red_Arg_Comm_Post-{postid}-{filename_date}.csv\")\n",
        "    archivo = f\"Red_Arg_Comm_Post-{postid}-{filename_date}.csv\""
      ],
      "metadata": {
        "id": "bLHxm4aJhQnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fase #2: Preprocesamiento"
      ],
      "metadata": {
        "id": "S2tMumyniZTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define el path del archivo CVS que contiene todo el Dataset unificado y se carga el CSV en un Dataframe\n",
        "file_path = 'https://raw.githubusercontent.com/luchamaria13/TIF2/refs/heads/main/Red_Arg_Dataset%20-%20Red_Arg_Dataset.csv.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Archivo cargado exitosamente. Muestra de datos:\")\n",
        "    print(df.head())\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el archivo: {e}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-YXlZJoLicdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fase #3: Procesamiento\n",
        "\n",
        "3.1. Identificación de post más votados y más comentados\n",
        "Pese a que este no es el primer objetivo específico, teniendo en cuenta que es el procedimiento con menor nivel de complejidad es el que se lleva a cabo primero"
      ],
      "metadata": {
        "id": "-8T5nGvonibx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para limpiar texto\n",
        "def limpiar_texto(texto):\n",
        "    if not isinstance(texto, str) or not texto.strip():\n",
        "        return ''\n",
        "    texto = texto.lower()  # Convertir a minúsculas\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Quitar caracteres especiales\n",
        "    texto = re.sub(r'\\d+', '', texto)  # Quitar números\n",
        "    texto = ' '.join(texto.split())  # Eliminar espacios extras\n",
        "    #texto = eliminar_tildes(texto)  # Eliminar tilde #\n",
        "    if len(texto.split()) < 4:  # Ignorar textos demasiado cortos\n",
        "        return ''\n",
        "\n",
        "    # Lista de stopwords\n",
        "    stop_words = set(stopwords.words('spanish'))\n",
        "    stop_words.update([\n",
        "        \"acá\",\"link\",\"jar\",\"jajaj\", \"jajajaj\", \"jajajajajar\", \"jajajajaja\", \"kjjj\", \"jajaja\", \"jajajja\", \"kjj\",\n",
        "        \"eo\", \"eto\", \"eta\", \"u\", \"che\", \"reddit\", \"the\", \"only\", \"get\", \"meta\", \"by\", \"removed\",\n",
        "        \"ear\", \"ete\", \"jajajajajajaja\", \"que\", \"de\", \"el\", \"por\", \"se\", \"está\", \"lo\", \"en\",\n",
        "        \"si\", \"https\", \"preview\", \"redd\", \"it\", \"giphy\", \"gif\", \"jpeg\", \"webp\", \"www\", \"png\",\n",
        "        \"format\", \"com\", \"width\", \"y\", \"pero\", \"como\", \"solo\", \"ma\", \"sino\", \"q\", \"ahi\", \"jajaja\",\n",
        "        \"jaja\", \"hacer\", \"mas\", \"porque\", \"muy\", \"donde\", \"siempre\", \"ya\", \"uno\", \"hace\",\"the\",\"one\",\"atchv\",\"youtube\",\"xd\",\"jajs\",\"username\",\"usuario\",\"oh\",\"ho\",\"di\",\"etc\",\"xb\"\n",
        "    ])\n",
        "\n",
        "    # Eliminar stopwords del texto\n",
        "    palabras = [palabra for palabra in texto.split() if palabra not in stop_words]\n",
        "    return \" \".join(palabras)"
      ],
      "metadata": {
        "id": "63Eto3USm4Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#En primer lugar se crea una función para analizar los upvotes y downvotes de los post y los comentarios del dataset\n",
        "def analizar_upvotes(df):\n",
        "    # Análisis estadístico descriptivo de upvotes\n",
        "    resumen_upvotes = df['upv'].describe()\n",
        "\n",
        "    # Distribución de upvotes por rango\n",
        "    df['rango_upvotes'] = pd.cut(df['upv'],\n",
        "        bins=[-float('inf'), 0, 10, 50, 100, 500, float('inf')],\n",
        "        labels=['Negativos', '1-10', '11-50', '51-100', '101-500', '500+'])\n",
        "\n",
        "    # Conteo de posts por rango de upvotes\n",
        "    distribucion_upvotes = df['rango_upvotes'].value_counts().sort_index()\n",
        "\n",
        "    # Porcentaje de posts por rango de upvotes\n",
        "    porcentaje_upvotes = (distribucion_upvotes / len(df) * 100).round(2)\n",
        "\n",
        "    # Top 10 posts con más upvotes\n",
        "    top_10_upvotes = df.nlargest(10, 'upv')[['upv', 'text']]\n",
        "\n",
        "    # Visualizaciones\n",
        "    plt.figure(figsize=(15,10))\n",
        "\n",
        "    # Subplot 1: Distribución de Upvotes\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.barplot(x=distribucion_upvotes.index, y=distribucion_upvotes.values)\n",
        "    plt.title('Distribución de Upvotes')\n",
        "    plt.xlabel('Rango de Upvotes')\n",
        "    plt.ylabel('Número de Posts')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Subplot 2: Porcentaje de Upvotes\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.barplot(x=porcentaje_upvotes.index, y=porcentaje_upvotes.values)\n",
        "    plt.title('Porcentaje de Posts por Rango de Upvotes')\n",
        "    plt.xlabel('Rango de Upvotes')\n",
        "    plt.ylabel('Porcentaje de Posts')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Gráfico de Top 10 posts con más upvotes\n",
        "    plt.figure(figsize=(14,7))\n",
        "    bars = plt.barh(\n",
        "    top_10_upvotes['text'].str[:50],\n",
        "    top_10_upvotes['upv'],\n",
        "    color='skyblue'\n",
        ")\n",
        "    plt.title('Top 10 Posts con Más Upvotes')\n",
        "    plt.xlabel('Número de upvotes')\n",
        "    plt.ylabel('Texto del post')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Ejemplo de uso\n",
        "resultados_upvotes = analizar_upvotes(df)\n",
        "\n",
        "# Filtrar posts con votos negativos y ordenarlos\n",
        "# Filtrar posts con votos negativos y ordenarlos\n",
        "top_10_downvotes = df[df['upv'] < 0].nsmallest(10, 'upv')\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "bars = plt.barh(\n",
        "    top_10_downvotes['text'].str[:50],\n",
        "    abs(top_10_downvotes['upv']),  # Valor absoluto para longitud de barra\n",
        "    color='salmon'\n",
        ")\n",
        "plt.title('Top 10 Posts con Más Downvotes en Reddit Argentina', fontsize=15)\n",
        "plt.xlabel('Número de Downvotes', fontsize=12)\n",
        "plt.ylabel('Extracto del Texto', fontsize=12)\n",
        "\n",
        "# Añadir etiquetas con el número de votos al final de cada barra\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    plt.text(\n",
        "        width,\n",
        "        bar.get_y() + bar.get_height()/2.,\n",
        "        f' {int(width):,} downvotes',\n",
        "        va='center',\n",
        "        ha='left',\n",
        "        fontsize=10,\n",
        "        color='darkred'\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G5ER7AwtuWYl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrar solo los comentarios\n",
        "comentarios = df[df['type'].str.strip().str.lower() == 'comment']\n",
        "\n",
        "# Contar el número de comentarios por cada post (usando context)\n",
        "conteo_comentarios = comentarios.groupby('context').size().reset_index(name='n_comentarios')\n",
        "\n",
        "# Ordenar por número de comentarios en orden descendente\n",
        "conteo_comentarios = conteo_comentarios.sort_values(by='n_comentarios', ascending=False)\n",
        "\n",
        "# Seleccionar los 10 posts con más comentarios\n",
        "top_10_posts = conteo_comentarios.head(10)\n",
        "\n",
        "# Filtrar los posts principales (type == 'post')\n",
        "posts = df[df['type'].str.strip().str.lower() == 'post']\n",
        "\n",
        "# Combinar la información de los textos con el conteo de comentarios\n",
        "tabla_completa = top_10_posts.merge(posts, left_on='context', right_on='id')\n",
        "\n",
        "# Seleccionar columnas relevantes: ID del post, texto y número de comentarios\n",
        "tabla_completa = tabla_completa[['id', 'text', 'n_comentarios']]\n",
        "\n",
        "# Mostrar la tabla\n",
        "print(tabla_completa)\n",
        "# Guardar la tabla en un archivo CSV\n",
        "tabla_completa.to_csv(\"top_10_posts.csv\", index=False, encoding='utf-8-sig')\n",
        "\n",
        "# Gráfica de los 10 posts más comentados\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(tabla_completa['id'].astype(str), tabla_completa['n_comentarios'], color='skyblue')\n",
        "plt.title('Top 10 Posts con Más Comentarios', fontsize=14)\n",
        "plt.xlabel('ID del Post', fontsize=12)\n",
        "plt.ylabel('Número de Comentarios', fontsize=12)\n",
        "plt.xticks(rotation=45, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a-mHutFJtQQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2. Identificación de los elementos del discurso libertario en el corpus.\n",
        "La estrategia que se utilizó en este caso fue el uso de bag words. Así, se construyó un dicionario de palabras presentes en el dataset que hiciera parte de las categorias que componen el discurso libertario (identificadas en la revisión de la literatura)"
      ],
      "metadata": {
        "id": "pkZ_j17GzGC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir palabras clave para cada categoría\n",
        "palabras_clave = {\n",
        "    \"Posturas en contra de la casta política\": [\n",
        "        \"CASTA\", \"CASTAA\", \"Ks CASTA\", \"KUKAS CASTA\", \"CASTA ZURDA\",\n",
        "        \"CASTA MAFIOSA\", \"VERDADERA CASTA\", \"PAGA LA CASTA\",\n",
        "        \"CASTA DE LADRONES\", \"PERONISMO CAMUFLADO CASTA\", \"LOS K CASTA\",\n",
        "        \"TIEMBLA LA CASTA\", \"CASTA MUCHACHES PERONISTES\",\n",
        "        \"CASTA VIOLADORES CORRUPTOS\", \"BENEFICIO DE CASTA\"\n",
        "    ],\n",
        "    \"Rechazo anti kirchnerismo-peronismo-izquierdas\": [\n",
        "        \"K\", \"PERIODISTAS K\", \"PERONISTA ASOCIACIÓN ILÍCITA\",\n",
        "        \"K CHORROS\", \"IZQUIERDA TERRORISTA ANTIDEMOCRÁTICA\",\n",
        "        \"PERONISTA TOMA PODER\", \"PERONISTAS IZQUIERDA GOLPISTA\",\n",
        "        \"NO SOY K PERO\", \"PERONCHO MEDIOS K\", \"LOS KA PERONISTA\",\n",
        "        \"LOS K\", \"K SOCIALISTAS\", \"IZQUIERDA GOLPISTA\", \"KAKA\",\n",
        "        \"KUKA\", \"KUKAS\", \"PRE-KUKAS KIRCHO\", \"KIRCHOS\", \"KIRCHNERATO\",\n",
        "        \"KUKARACHA\", \"KUKARACHOS\", \"KUKARDO\", \"KUKARDOS\", \"KUKALOIDE\",\n",
        "        \"KUKANETA\", \"KUKALISMO\", \"KUKAMONGA\", \"KUKATRILES\",\n",
        "        \"INKUKAY\", \"KUKATRAP\", \"PERONCHO\", \"PERONCHOS\", \"CAMARRRADA\",\n",
        "        \"ZURDO\", \"ZURDOS\", \"ZURDAJE\", \"SURDITO\", \"NSKP\", \"MONTONEROS\",\n",
        "        \"GUERRILLEROS\", \"IZQUIERDA\", \"PERONISTA\", \"PERONISTAS\"\n",
        "    ],\n",
        "    \"Anticolectivismo/Racismo/Xenofobia\": [\n",
        "        \"GORDOS PIQUETEROS\", \"PERUANOS COME PALOMAS\",\n",
        "        \"BOLIVIANOS PARAGUAYOS DEPREDADORES\", \"SIMIOS DEL CONGOURBANO\",\n",
        "        \"PLANES KIRCHERISMO PERONISMO\", \"ÑOQUIS SIMIOS TIRAPIEDRAS\",\n",
        "        \"PERUANO OTAKU\", \"PLANERO\", \"MAPUTRUCHOS\", \"MAPUCHES\",\n",
        "        \"PERUANOS\", \"PERUANO\", \"COMECHINGONES\", \"MACACOS\", \"CONGOURBANO\"\n",
        "    ],\n",
        "    \"Antifeminismo/anti población LGTBI\": [\n",
        "        \"PIBARDOS LGVT\", \"TRAVA\", \"FEMINAZIS\", \"GAY\", \"TROLO\",\n",
        "        \"PUTO\", \"TORTA\", \"PIBIS\"\n",
        "    ],\n",
        "    \"A favor de la defensa de ley y orden\": [\n",
        "        \"PENA\", \"PENAS\", \"REPRESIÓN\", \"DELITO ADULTO PENA ADULTO\",\n",
        "        \"REPRESIÓN ORDEN PÚBLICO\", \"PENAS SON CHISTE\",\n",
        "        \"VERDADERA CORRECTA REPRESIÓN\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# El resto del código permanece igual que en la versión anterior\n",
        "# Inicializar un contador para almacenar las frecuencias por categoría\n",
        "frecuencias = {categoria: 0 for categoria in palabras_clave.keys()}\n",
        "\n",
        "# Contar la frecuencia de palabras clave en cada categoría\n",
        "for categoria, palabras in palabras_clave.items():\n",
        "    # Crear un patrón regex para las palabras clave sin grupos de captura\n",
        "    patron = r'\\b(?:' + '|'.join(palabras) + r')\\b'\n",
        "    # Sumar la cantidad de coincidencias en todo el dataset\n",
        "    frecuencias[categoria] = df['text'].str.count(patron).sum()\n",
        "\n",
        "# Mostrar frecuencias por categoría\n",
        "print(\"Frecuencias por categoría:\")\n",
        "for categoria, frecuencia in frecuencias.items():\n",
        "    print(f\"{categoria}: {frecuencia}\")\n",
        "\n",
        "# Visualizar las frecuencias en un gráfico de barras\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(frecuencias.keys(), frecuencias.values(), color='skyblue')\n",
        "plt.title('Frecuencia de palabras clave por categoría', fontsize=14)\n",
        "plt.xlabel('Categorías', fontsize=12)\n",
        "plt.ylabel('Frecuencia', fontsize=12)\n",
        "plt.xticks(rotation=45, fontsize=10, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Análisis detallado por texto: agregar columnas para cada categoría\n",
        "for categoria, palabras in palabras_clave.items():\n",
        "    patron = r'\\b(?:' + '|'.join(palabras) + r')\\b'\n",
        "    # Crear una columna booleana que indica si un texto contiene palabras clave de la categoría\n",
        "    df[categoria] = df['text'].str.contains(patron, regex=True)\n",
        "\n",
        "# Contar la frecuencia de palabras específicas de cada categoría en todo el dataset\n",
        "for categoria, palabras in palabras_clave.items():\n",
        "    word_counts = Counter()\n",
        "\n",
        "    # Contar cada palabra de la categoría en los textos\n",
        "    for palabra in palabras:\n",
        "        word_counts[palabra] = df['text'].str.count(palabra).sum()\n",
        "\n",
        "    # Mostrar las 10 palabras más comunes\n",
        "    top_words = word_counts.most_common(10)\n",
        "\n",
        "    # Crear un gráfico de barras para las 10 palabras más frecuentes\n",
        "    if top_words:\n",
        "        palabras, frecuencias = zip(*top_words)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(palabras, frecuencias, color='skyblue')\n",
        "        plt.title(f'Las 10 palabras más frecuentes en la categoría {categoria}', fontsize=14)\n",
        "        plt.xlabel('Palabras', fontsize=12)\n",
        "        plt.ylabel('Frecuencia', fontsize=12)\n",
        "        plt.xticks(rotation=45, fontsize=10, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Exportar el dataset con las nuevas columnas\n",
        "df.to_csv(\"analisis_categorias.csv\", index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"El análisis detallado se ha exportado como 'analisis_categorias.csv'.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "n1pz09GmKdgM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3. Identificación de los tópicos presentes en el dataset\n",
        "Para esto se utilizó una estrategia de topic modeling."
      ],
      "metadata": {
        "id": "vMWjGkgn5VIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Primero armamos un diccionario con los 10 post más populares del subrredit Argentina#\n",
        "post_dict = {\"id\": [],\n",
        "             \"context\": [],\n",
        "             \"text\": [],\n",
        "             \"upv\": [],\n",
        "             \"author\": [],\n",
        "             \"date\": [],\n",
        "             \"type\": [],\n",
        "             }\n",
        "arg_subreddit = reddit.subreddit('Argentina')\n",
        "for post in arg_subreddit.top(limit=10, time_filter='day'):\n",
        "    post_dict[\"id\"].append(post.id)\n",
        "    post_dict[\"context\"].append(post.id)\n",
        "    post_dict[\"text\"].append(post.title)\n",
        "    post_dict[\"upv\"].append(post.score)\n",
        "    post_dict[\"author\"].append(post.author)\n",
        "    post_dict[\"date\"].append(filename_date)\n",
        "    post_dict['type'].append('post')"
      ],
      "metadata": {
        "id": "2qDo42SWdZ9o",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definición de una muestra de los textos lematizados para probar el modelo#\n",
        "sample_df = df.sample(frac=1, random_state=42)\n",
        "documentos = sample_df['text_lematizado'].tolist()"
      ],
      "metadata": {
        "id": "_vnr8SBk7Ks-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crear diccionario y corpus\n",
        "dictionary = corpora.Dictionary([doc.split() for doc in sample_df['text_lematizado']])\n",
        "corpus = [dictionary.doc2bow(doc.split()) for doc in sample_df['text_lematizado']]"
      ],
      "metadata": {
        "id": "i28jf5mp7tR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the BERTopic model\n",
        "topic_model = BERTopic(language=\"spanish\", calculate_probabilities=True, verbose=True)\n",
        "# Fit the model to your data\n",
        "topics, probs = topic_model.fit_transform(sample_df['text_lematizado'].tolist())\n",
        "topic_info = topic_model.get_topic_info()\n",
        "topic_info_df = topic_info.copy()\n",
        "topic_info_df['Palabras_clave'] = topic_info_df['Topic'].apply(lambda topic_id: ', '.join([word for word, _ in topic_model.get_topic(topic_id)][:10]) if topic_id != -1 else '')"
      ],
      "metadata": {
        "id": "-oCZRTXY706F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para lematizar texto\n",
        "import re\n",
        "def lematizar_texto(texto):\n",
        "    doc = nlp(texto)\n",
        "    tokens_lematizados = [token.lemma_ for token in doc if token.is_alpha]  # Solo palabras\n",
        "    return \" \".join(tokens_lematizados)\n",
        "\n",
        "# Aplicar funciones de limpieza y lematización al dataset\n",
        "if 'text' in df.columns:\n",
        "    df['text_lematizado'] = df['text'].apply(limpiar_texto).apply(lematizar_texto)\n",
        "    texto = df['text_lematizado'].tolist()  # Convertir columna en lista\n",
        "    print(\"Texto procesado:\")\n",
        "    print(texto[:5])  # Muestra los primeros 5 textos procesados\n",
        "    num_entradas = len(df[df['text_lematizado'] != ''])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vPLlH2F55xm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analizar_temas_detallado(topic_model, n_temas=10):\n",
        "    print(\"\\nAnálisis detallado de los temas principales:\")\n",
        "    for topic_id in range(min(n_temas, len(topic_model.get_topic_info()))):\n",
        "        if topic_id != -1:\n",
        "            print(f\"\\nTema {topic_id}:\")\n",
        "            palabras = topic_model.get_topic(topic_id)\n",
        "            print(\"Palabras clave:\", \", \".join([word for word, _ in palabras[:10]]))\n",
        "            docs = topic_model.get_representative_docs(topic_id)\n",
        "            print(\"\\nEjemplos de documentos:\")\n",
        "            for doc in docs[:3]:\n",
        "                print(f\"- {doc[:200]}...\")\n",
        "            size = topic_model.get_topic_sizes()[topic_id]\n",
        "            print(f\"\\nCantidad de documentos en este tema: {size}\")"
      ],
      "metadata": {
        "id": "PoHyhBfk8DxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLjaiqx6nLhk"
      },
      "outputs": [],
      "source": [
        "def visualizar_palabras_clave(topic_model, n_temas=10, n_palabras=10):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    # Definir stopwords dentro de la función\n",
        "    stop_words = set(stopwords.words('spanish'))\n",
        "    stop_words.update([\n",
        "        \"jajaj\", \"jajajaj\", \"jajajajajar\", \"jajajajaja\", \"kjjj\", \"jajaja\", \"jajajja\", \"kjj\",\n",
        "        \"eo\", \"eto\", \"eta\", \"u\", \"che\", \"reddit\", \"the\", \"only\", \"get\", \"meta\", \"by\", \"removed\",\n",
        "        \"ear\", \"ete\", \"jajajajajajaja\", \"que\", \"de\", \"el\", \"por\", \"se\", \"está\", \"lo\", \"en\",\n",
        "        \"si\", \"https\", \"preview\", \"redd\", \"it\", \"giphy\", \"gif\", \"jpeg\", \"webp\", \"www\", \"png\",\n",
        "        \"format\", \"com\", \"width\", \"y\", \"pero\", \"como\", \"solo\", \"ma\", \"sino\", \"q\", \"ahi\", \"jajaja\",\n",
        "        \"jaja\", \"hacer\", \"mas\", \"porque\", \"muy\", \"donde\", \"siempre\", \"ya\", \"uno\", \"hace\",\"atchv\",\"youtube\",\"xd\",\"jajs\",\"username\",\"usuario\"\n",
        "    ])\n",
        "\n",
        "\n",
        "    fig, axes = plt.subplots(min(n_temas, 5), 2, figsize=(15, 4*min(n_temas, 5)))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, topic_id in enumerate(range(min(n_temas, len(topic_model.get_topic_info())))):\n",
        "        if topic_id != -1 and idx < len(axes):\n",
        "            palabras = topic_model.get_topic(topic_id)\n",
        "\n",
        "            # Filtra las palabras clave aquí\n",
        "            palabras_clave_filtradas = [(word, score) for word, score in palabras if word not in stop_words]\n",
        "\n",
        "            # Usa las palabras clave filtradas para la visualización\n",
        "            palabras = dict(palabras_clave_filtradas[:n_palabras])\n",
        "\n",
        "            axes[idx].barh(list(palabras.keys()), list(palabras.values()))\n",
        "            axes[idx].set_title(f'Tema {topic_id}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualizar_palabras_clave(topic_model)\n"
      ],
      "metadata": {
        "id": "FZ-ztBNuLPOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# Calculate coherence using Gensim's CoherenceModel\n",
        "topic_words = [topic_model.get_topic(topic_id) for topic_id in range(len(topic_model.get_topic_info())) if topic_id != -1]\n",
        "topic_words = [topic for topic in topic_words if not isinstance(topic, bool)]\n",
        "topic_words = [[word for word, _ in topic] for topic in topic_words]\n",
        "\n",
        "# Create a Gensim dictionary\n",
        "dictionary = Dictionary([doc.split() for doc in documentos])\n",
        "\n",
        "# Convert topic words to token IDs\n",
        "topic_words_ids = [[dictionary.token2id[word] for word in topic if word in dictionary.token2id] for topic in topic_words]\n",
        "\n",
        "# Calculate the coherence score (C_v)\n",
        "coherence_model = CoherenceModel(\n",
        "    topics=topic_words_ids,\n",
        "    texts=[doc.split() for doc in documentos],\n",
        "    dictionary=dictionary,\n",
        "    coherence='c_v'\n",
        ")\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print(f\"Coherence Score (C_v): {coherence_score}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aceTpFpQytB",
        "outputId": "3dba44ad-6b9f-44ea-c595-aca15c822255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score (C_v): 0.3316138385826245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "def calcular_silhouette_score(documentos, topics):\n",
        "    # Vectorización de los documentos\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(documentos)\n",
        "\n",
        "    # Convertir matriz dispersa a densa\n",
        "    X_dense = X.toarray()\n",
        "\n",
        "    # Calcular Silhouette Score\n",
        "    try:\n",
        "        silhouette_avg = silhouette_score(X_dense, topics)\n",
        "        return silhouette_avg\n",
        "    except Exception as e:\n",
        "        print(f\"Error al calcular Silhouette Score: {e}\")\n",
        "        return None\n",
        "\n",
        "# Calcular Silhouette Score para BERTopic\n",
        "silhouette_bertopic = calcular_silhouette_score(documentos, topics)\n",
        "print(f\"Silhouette Score de BERTopic: {silhouette_bertopic}\")"
      ],
      "metadata": {
        "id": "PoFeLpTyh_-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig_topics = topic_model.visualize_topics()\n",
        "fig_topics.show()\n",
        "\n",
        "fig_hierarchy = topic_model.visualize_hierarchy()\n",
        "fig_hierarchy.show()\n"
      ],
      "metadata": {
        "id": "NWoptWlFviH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calcular_perplexity_bertopic(modelo_bertopic, topics):\n",
        "\n",
        "    _, counts = np.unique(topics, return_counts=True)\n",
        "    probabilidades = counts / len(topics)\n",
        "    entropia = -np.sum(probabilidades * np.log2(probabilidades))\n",
        "\n",
        "\n",
        "    perplexity = 2 ** entropia\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "perplexity_bertopic = calcular_perplexity_bertopic(topic_model, topics)\n",
        "print(f\"Perplexity de BERTopic: {perplexity_bertopic}\")"
      ],
      "metadata": {
        "id": "OxM_bNVgSvjz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}